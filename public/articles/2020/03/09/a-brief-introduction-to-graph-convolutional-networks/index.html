<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>A Brief Introduction to Graph Convolutional Networks | Depth-First</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Using graphs in machine learning.">
    <meta name="generator" content="Hugo 0.139.4">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Richard L. Apodaca">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/articles/2020/03/09/a-brief-introduction-to-graph-convolutional-networks/">
    

    <meta property="og:url" content="http://localhost:1313/articles/2020/03/09/a-brief-introduction-to-graph-convolutional-networks/">
  <meta property="og:site_name" content="Depth-First">
  <meta property="og:title" content="A Brief Introduction to Graph Convolutional Networks">
  <meta property="og:description" content="Using graphs in machine learning.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2020-03-09T14:00:00+00:00">
    <meta property="article:modified_time" content="2020-03-09T14:00:00+00:00">

  <meta itemprop="name" content="A Brief Introduction to Graph Convolutional Networks">
  <meta itemprop="description" content="Using graphs in machine learning.">
  <meta itemprop="datePublished" content="2020-03-09T14:00:00+00:00">
  <meta itemprop="dateModified" content="2020-03-09T14:00:00+00:00">
  <meta itemprop="wordCount" content="596">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="A Brief Introduction to Graph Convolutional Networks">
  <meta name="twitter:description" content="Using graphs in machine learning.">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Depth-First
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">A Brief Introduction to Graph Convolutional Networks</h1>
      
      <p class="tracked">
        By <strong>Richard L. Apodaca</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-03-09T14:00:00Z">March 9, 2020</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>The extensive use of graphs in chemistry to model both reactions and molecules creates challenges for machine learning. Most of the previous and ongoing research in the field solves the problem by translating molecular graphs into forms that fit cleanly within existing paradigms and tools: <a href="/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/">images</a>; <a href="/articles/2019/03/19/chemical-line-notations-for-deep-learning-deepsmiles-and-beyond/">text</a>; and binary fingerprints. Graph representations themselves seem out-of-place at first.</p>
<p>However, it&rsquo;s possible to work with graph representations more directly using <em>Graph Convolutional Networks</em> (GCNs). This article gives a quick introduction to the idea.</p>
<h1 id="circular-fingerprints">Circular Fingerprints</h1>
<p>If you&rsquo;re familiar with <a href="/articles/2019/01/11/extended-connectivity-fingerprints/">extended connectivity fingerprints</a> (aka ECFP or &ldquo;circular fingerprints&rdquo;) or Morgan&rsquo;s algorithm on which circular fingerprints are based, then graph convolutional networks will seem familiar. A circular fingerprint is a set of feature vectors in which each member represents an atom in a molecule.</p>
<p>An ECFP is characterized by its diameter of perception, or double the length of the shortest path between the focal atom and the atoms augmenting the feature vector. The simplest form, ECFP_0, disregards neighboring atoms. Its feature vectors would include individual atomic properties only such as: atomic number; isotope; degree; charge; and unsaturation. The next level, ECFP_2, sums the feature vectors of immediate neighbors with that of the central atom. ECFP_4 does the same, but for two layers of neighbors, and so on.</p>
<p>With each increase in the radius of perception, the atomic feature vectors of a circular fingerprint include more information about the rest of the molecule.</p>
<h1 id="message-passing">Message Passing</h1>
<p>Graph neural networks work on a similar principle called <em>message passing</em>. The procedure can be thought of as working through matrix operations. Given a graph with <em>N</em> nodes, the inputs to a GCN are:</p>
<ul>
<li>An <em>N</em>x<!-- raw HTML omitted -->F<!-- raw HTML omitted -->0<!-- raw HTML omitted --> feature matrix <em>X</em>, where <!-- raw HTML omitted -->F<!-- raw HTML omitted -->0<!-- raw HTML omitted --> is the number of initial node features; and</li>
<li>An <em>N</em>x<em>N</em> adjacency matrix.</li>
</ul>
<p>Each layer of the GCN defines a propagation rule, also in the form of a matrix. The propagation rule determines how inputs will be transformed before being sent to the next layer. Propagation rules can take many forms. The simplest does little more than multiply the incoming feature matrix by the adjacency matrix.</p>
<p>For example, consider the following directed graph <em>G</em>:</p>
<!-- raw HTML omitted -->
<p>It can be represented by the adjacency matrix <em>A</em>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>0 1 0 0
</span></span><span style="display:flex;"><span>0 0 1 1
</span></span><span style="display:flex;"><span>0 1 0 0
</span></span><span style="display:flex;"><span>1 0 1 0
</span></span></code></pre></div><p>A simple feature matrix might be composed of an ordered pair consisting of the node&rsquo;s index and a label equal to the additive inverse of this index. We could represent this as the following feature matrix <em>X</em>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>0   0
</span></span><span style="display:flex;"><span>1  -1
</span></span><span style="display:flex;"><span>2  -2
</span></span><span style="display:flex;"><span>3  -3
</span></span></code></pre></div><p>Imagine our propagation rule is to simply multiply <em>A</em> by <em>X</em>. Doing so yields:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-console" data-lang="console"><span style="display:flex;"><span>1  -1
</span></span><span style="display:flex;"><span>5  -5
</span></span><span style="display:flex;"><span>1  -1
</span></span><span style="display:flex;"><span>2  -2
</span></span></code></pre></div><p>Notice how the resulting matix is the same one we&rsquo;d construct if asked to report the sum of indices for neighboring nodes in the first column and the sum of weights for neighboring nodes in the second column. Multiplying the resulting matrix again by <em>A</em> would give a result that included information about features one node more distant. And so on.</p>
<p>Although this approach is too simple by itself to yield anything useful computationally, it does illuatrate how the message passing idea can be reduced to practice through matrix arithmetic. For more, see the excellent introduction <em><a href="https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780">How to do Deep Learning on Graphs with Graph Convolutional Networks</a></em>.</p>
<!-- raw HTML omitted -->
<p>A related presentation from Microsoft is also worth watching.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Message passing bears a striking similarity to Morgan&rsquo;s algorithm and the construction of circular fingerprints. The process forms the basis of Graph Convolutional Networks, a powerful method for using graphs in machine learning.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Depth-First 2024 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
