<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Open Benchmarks for Cheminformatics | Depth-First</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Yesterday&rsquo;s post on cheminformatics benchmarking generated a number of interesting comments, both here and on a similar article posted to Egon Willighagen&rsquo;s blog.
One thing the discussion highlights is the need for a suite of benchmarks specifically aimed at comparing the performance of diverse cheminformatics toolkits under controlled conditions. Toward this end, Egon has set up a GitHub project called cheminfbenchmark, with my own fork of it appearing here.
If you wanted to create a fair and balanced benchmark suite, how would you do it? What tests would you include, how would you run them, how would you select the test data, and how would you report the results?">
    <meta name="generator" content="Hugo 0.139.4">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    
      <meta name="author" content = "Richard L. Apodaca">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://depth-first.com:1313/articles/2008-12-05-open-benchmarks-for-cheminformatics/">
    

    <meta property="og:url" content="https://depth-first.com:1313/articles/2008-12-05-open-benchmarks-for-cheminformatics/">
  <meta property="og:site_name" content="Depth-First">
  <meta property="og:title" content="Open Benchmarks for Cheminformatics">
  <meta property="og:description" content="Yesterday’s post on cheminformatics benchmarking generated a number of interesting comments, both here and on a similar article posted to Egon Willighagen’s blog.
One thing the discussion highlights is the need for a suite of benchmarks specifically aimed at comparing the performance of diverse cheminformatics toolkits under controlled conditions. Toward this end, Egon has set up a GitHub project called cheminfbenchmark, with my own fork of it appearing here.
If you wanted to create a fair and balanced benchmark suite, how would you do it? What tests would you include, how would you run them, how would you select the test data, and how would you report the results?">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="articles">
    <meta property="article:published_time" content="2008-12-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2008-12-05T00:00:00+00:00">

  <meta itemprop="name" content="Open Benchmarks for Cheminformatics">
  <meta itemprop="description" content="Yesterday’s post on cheminformatics benchmarking generated a number of interesting comments, both here and on a similar article posted to Egon Willighagen’s blog.
One thing the discussion highlights is the need for a suite of benchmarks specifically aimed at comparing the performance of diverse cheminformatics toolkits under controlled conditions. Toward this end, Egon has set up a GitHub project called cheminfbenchmark, with my own fork of it appearing here.
If you wanted to create a fair and balanced benchmark suite, how would you do it? What tests would you include, how would you run them, how would you select the test data, and how would you report the results?">
  <meta itemprop="datePublished" content="2008-12-05T00:00:00+00:00">
  <meta itemprop="dateModified" content="2008-12-05T00:00:00+00:00">
  <meta itemprop="wordCount" content="109">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Open Benchmarks for Cheminformatics">
  <meta name="twitter:description" content="Yesterday’s post on cheminformatics benchmarking generated a number of interesting comments, both here and on a similar article posted to Egon Willighagen’s blog.
One thing the discussion highlights is the need for a suite of benchmarks specifically aimed at comparing the performance of diverse cheminformatics toolkits under controlled conditions. Toward this end, Egon has set up a GitHub project called cheminfbenchmark, with my own fork of it appearing here.
If you wanted to create a fair and balanced benchmark suite, how would you do it? What tests would you include, how would you run them, how would you select the test data, and how would you report the results?">

	
  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Depth-First
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Open Benchmarks for Cheminformatics</h1>
      
      <p class="tracked">
        By <strong>Richard L. Apodaca</strong>
      </p>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2008-12-05T00:00:00Z">December 5, 2008</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Yesterday&rsquo;s post on <a href="/articles/2008/12/04/choose-java-for-speed">cheminformatics benchmarking</a> generated a number of interesting comments, both here and <a href="http://chem-bla-ics.blogspot.com/2008/12/who-says-java-is-not-fast.html">on a similar article</a> posted to <a href="http://chem-bla-ics.blogspot.com/">Egon Willighagen&rsquo;s blog</a>.</p>
<p>One thing the discussion highlights is the need for a suite of benchmarks specifically aimed at comparing the performance of diverse cheminformatics toolkits under controlled conditions. Toward this end, Egon has set up a <a href="/articles/2008/11/25/goodbye-subversion-hello-git-and-github">GitHub</a> project called <a href="http://github.com/egonw/cheminfbenchmark/tree">cheminfbenchmark</a>, with my own fork of it appearing <a href="http://github.com/rapodaca/cheminfbenchmark/">here</a>.</p>
<p>If you wanted to create a fair and balanced benchmark suite, how would you do it? What tests would you include, how would you run them, how would you select the test data, and how would you report the results?</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://depth-first.com:1313/" >
    &copy;  Depth-First 2024 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
